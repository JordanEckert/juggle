% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/METRICS.R
\name{auc}
\alias{auc}
\title{AUC of the ROC curve for two class setting.}
\usage{
auc(predict, actual)
}
\arguments{
\item{predict}{A vector of predicted class labels. Must be labeled as 1 and 2.}

\item{actual}{A vector of actual class labels. Must be labeled as 1 and 2.}
}
\value{
AUC of the ROC curve. Values range from 0 to 1
}
\description{
\code{auc} calculates AUC of the ROC curve.
It evaluates the model's ability to distinguish between positive and negative
classes across all classification thresholds.

Best use case: Evaluating and comparing the overall discriminative power of models.
}
\examples{
n <- 1000
x1 <- runif(n, 1, 10)
x2 <- runif(n, 1, 10)
x <- cbind(x1, x2)
y <- as.factor(ifelse(3 < x1 & x1 < 7 & 3 < x2 & x2 < 7, "A", "B"))

#' # testing the performance
i_train <- sample(1:n, round(n*0.8))

x_train <- x[i_train,]
y_train <- y[i_train]

x_test <- x[-i_train,]
y_test <- y[-i_train]

m_pcccd <- pcccd(x = x_train, y = y_train, tau = 1)
p_pred <- classify_pcccd(pcccd = m_pcccd, newdata = x_test)

auc(as.numeric(as.factor(p_pred)), as.numeric(as.factor(y_test)))

}
\author{
Jordan Eckert
}
